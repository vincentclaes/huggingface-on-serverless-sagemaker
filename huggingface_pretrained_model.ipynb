{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Huggingface Model to Serverless Sagemaker\n",
    "\n",
    "- set the kernel to a pytorch kernel\n",
    "- load a model from huggingface hub\n",
    "- register the model\n",
    "\n",
    "### Links\n",
    "\n",
    "- https://huggingface.co/docs/sagemaker/inference#deploy-a-model-from-the-hub\n",
    "- https://www.youtube.com/watch?v=l9QZuazbzWM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker transformers --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from sagemaker.huggingface.model import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using Sagemaker SDK\n",
    "#### 1.1 Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "pretrained_classifier = pipeline(\"sentiment-analysis\")\n",
    "pretrained_classifier.save_pretrained(\"./model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n",
      "model.tar.gz\n",
      "pytorch_model.bin\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "vocab.txt\n"
     ]
    }
   ],
   "source": [
    "! cd model/ && tar zcvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "default_bucket = sess.default_bucket()\n",
    "model_path = f\"s3://{default_bucket}/sagemaker-studio/huggingface-on-serverless-sagemaker/distilbert-base-uncased-finetuned-sst-2-english/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: model/model.tar.gz to s3://sagemaker-eu-west-1-077590795309/sagemaker-studio/huggingface-on-serverless-sagemaker/distilbert-base-uncased-finetuned-sst-2-english/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp model/model.tar.gz $model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub model configuration <https://huggingface.co/models>\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=model_path,  # path to your trained SageMaker model                                                # configuration for loading model from Hub\n",
    "    role=role,                                              # IAM role with permissions to create an endpoint\n",
    "    transformers_version=\"4.12\",                             # Transformers version used\n",
    "    pytorch_version=\"1.9\",                                  # PyTorch version used\n",
    "    py_version='py38',                                      # Python version used\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Deploy a Model and call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "#     initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    serverless_inference_config=ServerlessInferenceConfig(memory_size_in_mb=1024, max_concurrency=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 ms, sys: 0 ns, total: 17.4 ms\n",
      "Wall time: 10.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998599290847778}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = {\n",
    "   \"inputs\": \"I really like this place!\"\n",
    "}\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-inference-2022-04-25-14-11-24-594'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Remove the serverless sagemaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker delete-endpoint --endpoint-name $predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Huggingface on Serverless Sagemaker deployed by Serverless.com\n",
    "\n",
    "- Using sagemaker sdk you can deploy an endpoint, but you can only reach a sagemaker endpoint from within an AWS account.\n",
    "- Typically you want to expose your model to the outside world, another network\n",
    "- You want to deploy yoouor models from IAC.\n",
    "\n",
    "#### 2.1 create a sagemaker moodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-inference-2022-04-25-14-18-11-101'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a sagemaker model\n",
    "# https://github.com/aws/sagemaker-python-sdk/blob/d635faff4ac54f80465f7bc7f3181f67336e249a/src/sagemaker/model.py#L261\n",
    "# Maybe not the best way to create a sagemaker model, but i didn't found a better way.\n",
    "\n",
    "huggingface_model._create_sagemaker_model(instance_type=\"ml.m5.xlarge\", accelerator_type=None, tags=None)\n",
    "sagemaker_model_name = huggingface_model.name\n",
    "sagemaker_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 create a lambda to call the sagemaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lambda_handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lambda_handler.py\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "runtime_client = boto3.client(\"runtime.sagemaker\")\n",
    "sagemaker_endpoint_name = os.environ[\"SAGEMAKER_ENDPOINT_NAME\"]\n",
    "\n",
    "def handler(event, context):\n",
    "\n",
    "    print(f\"making a prediction on the text: {event['body']}\")\n",
    "    \n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        Body=event[\"body\"],\n",
    "        EndpointName=sagemaker_endpoint_name,\n",
    "        Accept=\"application/json\",\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    \n",
    "    print(f\"prediction: {response}\")\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(prediction)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 create a serverless.yml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write variables to a textfile\n",
    "# https://github.com/ipython/ipython/issues/6701#issuecomment-382640776\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate serverless.yml\n",
    "service: huggingface-on-serverless-sagemaker\n",
    "\n",
    "provider:\n",
    "  name: aws\n",
    "  region: eu-west-1 \n",
    "  runtime: python3.8\n",
    "  iam:\n",
    "    role:\n",
    "      managedPolicies: arn:aws:iam::aws:policy/AdministratorAccess\n",
    "\n",
    "\n",
    "functions:\n",
    "  huggingface:\n",
    "    handler: lambda_handler.handler\n",
    "    timeout: 120\n",
    "    memorySize: 128 \n",
    "    events:\n",
    "      - http:\n",
    "          path: prediction\n",
    "          method: post\n",
    "    environment:\n",
    "      SAGEMAKER_ENDPOINT_NAME: !GetAtt SageMakerEndpoint.EndpointName\n",
    "\n",
    "resources:\n",
    "  Resources:\n",
    "    SageMakerEndpointConfig:\n",
    "      Type: AWS::SageMaker::EndpointConfig\n",
    "      Properties:\n",
    "        ProductionVariants:\n",
    "          - ModelName: {sagemaker_model_name}\n",
    "#             InitialInstanceCount: 1\n",
    "            InitialVariantWeight: 1.0\n",
    "            VariantName: SageMakerModel\n",
    "            ServerlessConfig:\n",
    "              MaxConcurrency: 1\n",
    "              MemorySizeInMB: 4096\n",
    "\n",
    "    SageMakerEndpoint:\n",
    "      Type: AWS::SageMaker::Endpoint\n",
    "      Properties:\n",
    "        EndpointConfigName: !GetAtt SageMakerEndpointConfig.EndpointConfigName\n",
    "        EndpointName: huggingface-serverless-sagemaker-endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 open an AWS cloud shell and deploy the application\n",
    "\n",
    "```\n",
    "git clone https://github.com/vincentclaes/huggingface-on-serverless-sagemaker.git\n",
    "\n",
    "cd huggingface-on-serverless-sagemaker/\n",
    "\n",
    "npm install serverless\n",
    "\n",
    "/home/cloudshell-user/node_modules/serverless/bin/serverless.js deploy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Call the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\": \"Endpoint request timed out\"}CPU times: user 449 ms, sys: 111 ms, total: 560 ms\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!curl -d '{\"inputs\":\"some very much wow positive text!\"}' -H \"Content-Type: application/json\" -X POST  https://2kqraqs038.execute-api.eu-west-1.amazonaws.com/dev/prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 remove the stack\n",
    "\n",
    "```\n",
    "/home/cloudshell-user/node_modules/serverless/bin/serverless.js remove\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalDependencyException",
     "evalue": "An error occurred (InternalDependencyException) when calling the InvokeEndpoint operation: An exception occurred from internal dependency. Please contact customer support regarding request 4bfc2335-f7b9-450b-ac5f-fe09c4866de4.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalDependencyException\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-389f772da053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"huggingface-pytorch-inference-2022-02-01-13-58-48-937\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mAccept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mContentType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalDependencyException\u001b[0m: An error occurred (InternalDependencyException) when calling the InvokeEndpoint operation: An exception occurred from internal dependency. Please contact customer support regarding request 4bfc2335-f7b9-450b-ac5f-fe09c4866de4."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "data = '{\"inputs\":\"some very much wow positive text!\"}'\n",
    "\n",
    "response = boto3.client(\"runtime.sagemaker\").invoke_endpoint(\n",
    "        Body=data,\n",
    "#         EndpointName=\"huggingface-serverless-sagemaker-endpoint\",\n",
    "        EndpointName=\"huggingface-pytorch-inference-2022-02-01-13-58-48-937\",\n",
    "        Accept=\"application/json\",\n",
    "        ContentType=\"application/json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/1.8.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
