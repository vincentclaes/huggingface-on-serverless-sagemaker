{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Huggingface Model to Serverless Sagemaker\n",
    "\n",
    "- Load a model from huggingface hub and store on S3.\n",
    "- Register a huggingface model as a Sagemaker model.\n",
    "- Expose your HuggingFace Model to the Outside World.\n",
    "- Make predictions\n",
    "\n",
    "\n",
    "### Links\n",
    "\n",
    "- https://huggingface.co/docs/sagemaker/inference#deploy-a-model-from-the-hub\n",
    "- https://www.youtube.com/watch?v=l9QZuazbzWM\n",
    "\n",
    "## PREREQUISITES\n",
    "\n",
    "- Fork this repo, launch a Sagemaker (studio) notebook.\n",
    "- Clone the repo to the notebook.\n",
    "- Set the kernel to a pytorch kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker transformers --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sagemaker\n",
    "\n",
    "from transformers import pipeline\n",
    "from sagemaker.huggingface.model import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a HuggingFace Model and Store on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "pretrained_classifier = pipeline(\"sentiment-analysis\")\n",
    "pretrained_classifier.save_pretrained(\"./model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the pretrained model, to mimic a situation where \n",
    "<br>\n",
    "we finetuned on a pretrained model and saved the model to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n",
      "pytorch_model.bin\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "vocab.txt\n"
     ]
    }
   ],
   "source": [
    "! cd model/ && tar zcvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "default_bucket = sess.default_bucket()\n",
    "model_path = f\"s3://{default_bucket}/sagemaker-studio/huggingface-on-serverless-sagemaker/distilbert-base-uncased-finetuned-sst-2-english/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: model/model.tar.gz to s3://sagemaker-eu-west-1-077590795309/sagemaker-studio/huggingface-on-serverless-sagemaker/distilbert-base-uncased-finetuned-sst-2-english/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp model/model.tar.gz $model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Register HuggingFace Model as Sagemaker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=model_path,                                                \n",
    "    role=role,                              \n",
    "    transformers_version=\"4.12\",                            \n",
    "    pytorch_version=\"1.9\",                                  \n",
    "    py_version='py38', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-inference-2022-04-26-11-26-46-198'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a sagemaker model\n",
    "# https://github.com/aws/sagemaker-python-sdk/blob/d635faff4ac54f80465f7bc7f3181f67336e249a/src/sagemaker/model.py#L261\n",
    "# Maybe not the best way to create a sagemaker model, but i didn't found a better way.\n",
    "\n",
    "huggingface_model._create_sagemaker_model(instance_type=\"ml.m5.xlarge\", accelerator_type=None, tags=None)\n",
    "sagemaker_model_name = huggingface_model.name\n",
    "sagemaker_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expose your HuggingFace Model to the Outside World\n",
    "\n",
    "If you want to expose your model to the outside world,\n",
    "</br>\n",
    "you need to connect it to an API Gateway. We do this via a Lambda function.\n",
    "</br>\n",
    "</br>\n",
    "__API Gateway <-> Lambda <-> Sagemaker Endpoint__\n",
    "\n",
    "### 3.1  Lambda to forward the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lambda_handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lambda_handler.py\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "runtime_client = boto3.client(\"runtime.sagemaker\")\n",
    "sagemaker_endpoint_name = os.environ[\"SAGEMAKER_ENDPOINT_NAME\"]\n",
    "\n",
    "def handler(event, context):\n",
    "\n",
    "    print(f\"making a prediction on the text: {event['body']}\")\n",
    "    \n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        Body=event[\"body\"],\n",
    "        EndpointName=sagemaker_endpoint_name,\n",
    "        Accept=\"application/json\",\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    \n",
    "    prediction = response[\"Body\"].read()\n",
    "    print(f\"prediction: {prediction}\")\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': prediction\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Serverless.yml\n",
    "\n",
    "Define a serverless.yml file that describes the stack;\n",
    "\n",
    "- API Gateway + Lambda\n",
    "- Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write variables to a textfile\n",
    "# https://github.com/ipython/ipython/issues/6701#issuecomment-382640776\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate serverless.yml\n",
    "service: huggingface-on-serverless-sagemaker\n",
    "\n",
    "provider:\n",
    "  name: aws\n",
    "  region: eu-west-1 \n",
    "  runtime: python3.8\n",
    "  iam:\n",
    "    role:\n",
    "      managedPolicies: arn:aws:iam::aws:policy/AdministratorAccess\n",
    "\n",
    "\n",
    "functions:\n",
    "  huggingface:\n",
    "    handler: lambda_handler.handler\n",
    "    timeout: 120\n",
    "    memorySize: 128 \n",
    "    events:\n",
    "      - http:\n",
    "          path: prediction\n",
    "          method: post\n",
    "    environment:\n",
    "      SAGEMAKER_ENDPOINT_NAME: !GetAtt SageMakerEndpoint.EndpointName\n",
    "\n",
    "resources:\n",
    "  Resources:\n",
    "    SageMakerEndpointConfig:\n",
    "      Type: AWS::SageMaker::EndpointConfig\n",
    "      Properties:\n",
    "        ProductionVariants:\n",
    "          - ModelName: {sagemaker_model_name}\n",
    "#             InitialInstanceCount: 1\n",
    "            InitialVariantWeight: 1.0\n",
    "            VariantName: SageMakerModel\n",
    "            ServerlessConfig:\n",
    "              MaxConcurrency: 200\n",
    "              MemorySizeInMB: 1028\n",
    "\n",
    "    SageMakerEndpoint:\n",
    "      Type: AWS::SageMaker::Endpoint\n",
    "      Properties:\n",
    "        EndpointConfigName: !GetAtt SageMakerEndpointConfig.EndpointConfigName\n",
    "        EndpointName: huggingface-serverless-sagemaker-endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Deploy The Stack\n",
    "\n",
    "- First push your changes with the generated serverless.yml that contains the sagemaker model name you created.\n",
    "\n",
    "- Open an [AWS Cloudshell](https://docs.aws.amazon.com/cloudshell/latest/userguide/getting-started.html#launch-region-shell)\n",
    "\n",
    "```\n",
    "git clone my repo you forked.\n",
    "\n",
    "cd huggingface-on-serverless-sagemaker/\n",
    "\n",
    "npm install serverless\n",
    "\n",
    "/home/cloudshell-user/node_modules/serverless/bin/serverless.js deploy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make a Prediction\n",
    "\n",
    "Replace the endpoint with the one in the cell below.\n",
    "\n",
    "It should look  like\n",
    "\n",
    "     https://8xkeg6xv2b.execute-api.eu-west-1.amazonaws.com/dev/prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"label\":\"POSITIVE\",\"score\":0.9998674392700195}]CPU times: user 181 ms, sys: 44.4 ms, total: 226 ms\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!curl -d '{\"inputs\":\"some very much wow positive text!\"}' -H \"Content-Type: application/json\" -X POST  https://8xkeg6xv2b.execute-api.eu-west-1.amazonaws.com/dev/prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Burst of 10 000 (Serverless Sagemaker max concurrency = 200)\n",
    "\n",
    "- default max concurrency of a lambda function is 1000. \n",
    "- serverless sagemaker max concurrency is 200 \n",
    "- we update the serverless.yml too increase the concurrency ...\n",
    "\n",
    "        resources:\n",
    "        Resources:\n",
    "            SageMakerEndpointConfig:\n",
    "            Type: AWS::SageMaker::EndpointConfig\n",
    "            Properties:\n",
    "                ProductionVariants:\n",
    "                - ModelName: {sagemaker_model}\n",
    "                    InitialVariantWeight: 1.0\n",
    "                    VariantName: SageMakerModel\n",
    "                    ServerlessConfig:\n",
    "                    MaxConcurrency: 200 # <--- set the max concurrency here\n",
    "                    MemorySizeInMB: 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from joblib import Parallel, delayed\n",
    "import shlex\n",
    "import subprocess\n",
    "\n",
    "def process(i):\n",
    "    print(\".\",)\n",
    "    cmd = \"\"\"curl -d '{\"inputs\":\"some very much wow positive text!\"}' -H \"Content-Type: application/json\" -X POST  https://8xkeg6xv2b.execute-api.eu-west-1.amazonaws.com/dev/prediction\"\"\"\n",
    "    subprocess.check_call(shlex.split(cmd))\n",
    "    \n",
    "results = Parallel(n_jobs=100)(delayed(process)(i) for i in range(10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "- min latency is 20 ms.\n",
    "- max latency is 7100 ms.\n",
    "- average latency is 6500 ms when burst starts.\n",
    "- average latency is 80 ms when burst is caught up.\n",
    "- 100% of the requests were successful\n",
    "\n",
    "![assets/serverless-sagemaker-max-concurrency-1000.png](assets/serverless-sagemaker-max-concurrency-1000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Remove the Stack\n",
    "\n",
    "```\n",
    "/home/cloudshell-user/node_modules/serverless/bin/serverless.js remove\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/1.8.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
